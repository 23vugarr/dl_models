{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "xtest = pd.read_csv('X_test.csv')\n",
    "xtrain = pd.read_csv('X_train.csv')\n",
    "ytest = pd.read_csv(\"y_test.csv\")\n",
    "ytrain = pd.read_csv(\"y_train.csv\")\n",
    "\n",
    "df = pd.concat([xtest, xtrain], axis=0).sort_values(by='StudentID')\n",
    "yresult = pd.concat([ytrain, ytest], axis=0).sort_values(by='StudentID')\n",
    "\n",
    "df = pd.concat([df, yresult.drop(columns='StudentID')], axis=1).drop(columns='StudentID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "for i in df.columns:\n",
    "    if(df[i].dtype in ['object', 'str']):\n",
    "        df = pd.concat([pd.get_dummies(df[i], prefix=f'{i}').astype('int64'), df.drop(columns=i)], axis=1)\n",
    "\n",
    "len(df.columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "target = np.array(df['G3'])\n",
    "features = StandardScaler().fit_transform(np.array(df.drop(columns='G3')))\n",
    "\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(features, target, random_state=23, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(xtrain.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.02), loss=tf.keras.losses.mae, metrics=[tf.keras.metrics.mae])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 4.5605 - mean_absolute_error: 3.4589 - val_loss: 3.5337 - val_mean_absolute_error: 2.4265\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 3.9735 - mean_absolute_error: 2.8659 - val_loss: 3.3366 - val_mean_absolute_error: 2.2296\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 3.7087 - mean_absolute_error: 2.6050 - val_loss: 3.0361 - val_mean_absolute_error: 1.9368\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.5721 - mean_absolute_error: 2.4783 - val_loss: 2.9013 - val_mean_absolute_error: 1.8142\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.4978 - mean_absolute_error: 2.4173 - val_loss: 2.9276 - val_mean_absolute_error: 1.8546\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.4712 - mean_absolute_error: 2.4058 - val_loss: 2.8338 - val_mean_absolute_error: 1.7763\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.3311 - mean_absolute_error: 2.2813 - val_loss: 2.9670 - val_mean_absolute_error: 1.9256\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 3.2426 - mean_absolute_error: 2.2090 - val_loss: 2.6694 - val_mean_absolute_error: 1.6445\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 3.1718 - mean_absolute_error: 2.1552 - val_loss: 2.5155 - val_mean_absolute_error: 1.5080\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 3.0772 - mean_absolute_error: 2.0790 - val_loss: 2.4773 - val_mean_absolute_error: 1.4888\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 3.0329 - mean_absolute_error: 2.0537 - val_loss: 2.4330 - val_mean_absolute_error: 1.4637\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 3.0564 - mean_absolute_error: 2.0965 - val_loss: 2.3149 - val_mean_absolute_error: 1.3651\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 3.0131 - mean_absolute_error: 2.0729 - val_loss: 2.4230 - val_mean_absolute_error: 1.4930\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.9127 - mean_absolute_error: 1.9928 - val_loss: 2.2319 - val_mean_absolute_error: 1.3224\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.9875 - mean_absolute_error: 2.0879 - val_loss: 2.2135 - val_mean_absolute_error: 1.3242\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.8770 - mean_absolute_error: 1.9978 - val_loss: 2.2588 - val_mean_absolute_error: 1.3899\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.7391 - mean_absolute_error: 1.8798 - val_loss: 2.2142 - val_mean_absolute_error: 1.3650\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.7505 - mean_absolute_error: 1.9112 - val_loss: 2.2549 - val_mean_absolute_error: 1.4259\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.6811 - mean_absolute_error: 1.8617 - val_loss: 2.2323 - val_mean_absolute_error: 1.4229\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.7721 - mean_absolute_error: 1.9723 - val_loss: 2.1487 - val_mean_absolute_error: 1.3593\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.6317 - mean_absolute_error: 1.8514 - val_loss: 2.1330 - val_mean_absolute_error: 1.3627\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.5304 - mean_absolute_error: 1.7694 - val_loss: 2.0815 - val_mean_absolute_error: 1.3305\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.6235 - mean_absolute_error: 1.8823 - val_loss: 1.9685 - val_mean_absolute_error: 1.2376\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 2.5561 - mean_absolute_error: 1.8348 - val_loss: 2.1224 - val_mean_absolute_error: 1.4111\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 2.4925 - mean_absolute_error: 1.7905 - val_loss: 1.9865 - val_mean_absolute_error: 1.2942\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 2.5478 - mean_absolute_error: 1.8648 - val_loss: 1.9798 - val_mean_absolute_error: 1.3065\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.4899 - mean_absolute_error: 1.8261 - val_loss: 1.8954 - val_mean_absolute_error: 1.2414\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.4556 - mean_absolute_error: 1.8109 - val_loss: 1.9310 - val_mean_absolute_error: 1.2955\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.4649 - mean_absolute_error: 1.8384 - val_loss: 1.9929 - val_mean_absolute_error: 1.3757\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.3719 - mean_absolute_error: 1.7635 - val_loss: 1.8480 - val_mean_absolute_error: 1.2492\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.3612 - mean_absolute_error: 1.7712 - val_loss: 1.9149 - val_mean_absolute_error: 1.3342\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.3075 - mean_absolute_error: 1.7347 - val_loss: 1.8240 - val_mean_absolute_error: 1.2594\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.2458 - mean_absolute_error: 1.6896 - val_loss: 1.7643 - val_mean_absolute_error: 1.2168\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.3656 - mean_absolute_error: 1.8262 - val_loss: 1.8003 - val_mean_absolute_error: 1.2693\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.2245 - mean_absolute_error: 1.7011 - val_loss: 1.8270 - val_mean_absolute_error: 1.3113\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 2.1749 - mean_absolute_error: 1.6663 - val_loss: 1.7735 - val_mean_absolute_error: 1.2727\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.1775 - mean_absolute_error: 1.6846 - val_loss: 1.7511 - val_mean_absolute_error: 1.2663\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.1399 - mean_absolute_error: 1.6620 - val_loss: 1.7022 - val_mean_absolute_error: 1.2316\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.1392 - mean_absolute_error: 1.6756 - val_loss: 1.5972 - val_mean_absolute_error: 1.1405\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.2710 - mean_absolute_error: 1.8208 - val_loss: 1.6670 - val_mean_absolute_error: 1.2238\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.0747 - mean_absolute_error: 1.6381 - val_loss: 1.6152 - val_mean_absolute_error: 1.1854\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.1067 - mean_absolute_error: 1.6830 - val_loss: 1.6699 - val_mean_absolute_error: 1.2524\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.1851 - mean_absolute_error: 1.7732 - val_loss: 1.7449 - val_mean_absolute_error: 1.3390\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.0788 - mean_absolute_error: 1.6789 - val_loss: 1.6430 - val_mean_absolute_error: 1.2490\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.9811 - mean_absolute_error: 1.5928 - val_loss: 1.5719 - val_mean_absolute_error: 1.1898\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.0016 - mean_absolute_error: 1.6243 - val_loss: 1.6644 - val_mean_absolute_error: 1.2918\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 2.0222 - mean_absolute_error: 1.6543 - val_loss: 1.5310 - val_mean_absolute_error: 1.1680\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 2.0496 - mean_absolute_error: 1.6914 - val_loss: 1.5217 - val_mean_absolute_error: 1.1691\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.9438 - mean_absolute_error: 1.5955 - val_loss: 1.4583 - val_mean_absolute_error: 1.1148\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.9191 - mean_absolute_error: 1.5803 - val_loss: 1.4683 - val_mean_absolute_error: 1.1341\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.9099 - mean_absolute_error: 1.5799 - val_loss: 1.5577 - val_mean_absolute_error: 1.2324\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 2.0044 - mean_absolute_error: 1.6827 - val_loss: 1.4366 - val_mean_absolute_error: 1.1192\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 2.0709 - mean_absolute_error: 1.7574 - val_loss: 1.4784 - val_mean_absolute_error: 1.1686\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.8955 - mean_absolute_error: 1.5885 - val_loss: 1.4999 - val_mean_absolute_error: 1.1965\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.9232 - mean_absolute_error: 1.6232 - val_loss: 1.5858 - val_mean_absolute_error: 1.2890\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8341 - mean_absolute_error: 1.5404 - val_loss: 1.5022 - val_mean_absolute_error: 1.2120\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.9086 - mean_absolute_error: 1.6217 - val_loss: 1.5492 - val_mean_absolute_error: 1.2659\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8608 - mean_absolute_error: 1.5807 - val_loss: 1.3841 - val_mean_absolute_error: 1.1075\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.8610 - mean_absolute_error: 1.5879 - val_loss: 1.4727 - val_mean_absolute_error: 1.2029\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8284 - mean_absolute_error: 1.5613 - val_loss: 1.4333 - val_mean_absolute_error: 1.1694\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.9421 - mean_absolute_error: 1.6817 - val_loss: 1.4887 - val_mean_absolute_error: 1.2319\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.9069 - mean_absolute_error: 1.6525 - val_loss: 1.4160 - val_mean_absolute_error: 1.1640\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8569 - mean_absolute_error: 1.6074 - val_loss: 1.3832 - val_mean_absolute_error: 1.1363\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7844 - mean_absolute_error: 1.5400 - val_loss: 1.5004 - val_mean_absolute_error: 1.2590\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8476 - mean_absolute_error: 1.6085 - val_loss: 1.5180 - val_mean_absolute_error: 1.2816\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8706 - mean_absolute_error: 1.6360 - val_loss: 1.4424 - val_mean_absolute_error: 1.2104\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8326 - mean_absolute_error: 1.6020 - val_loss: 1.4056 - val_mean_absolute_error: 1.1765\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8057 - mean_absolute_error: 1.5783 - val_loss: 1.4395 - val_mean_absolute_error: 1.2145\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8562 - mean_absolute_error: 1.6331 - val_loss: 1.4746 - val_mean_absolute_error: 1.2532\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8768 - mean_absolute_error: 1.6572 - val_loss: 1.4885 - val_mean_absolute_error: 1.2710\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7978 - mean_absolute_error: 1.5808 - val_loss: 1.4161 - val_mean_absolute_error: 1.2002\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8894 - mean_absolute_error: 1.6749 - val_loss: 1.2857 - val_mean_absolute_error: 1.0729\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8461 - mean_absolute_error: 1.6346 - val_loss: 1.3713 - val_mean_absolute_error: 1.1619\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7811 - mean_absolute_error: 1.5729 - val_loss: 1.2852 - val_mean_absolute_error: 1.0783\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8382 - mean_absolute_error: 1.6331 - val_loss: 1.3614 - val_mean_absolute_error: 1.1575\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7144 - mean_absolute_error: 1.5116 - val_loss: 1.3578 - val_mean_absolute_error: 1.1560\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7319 - mean_absolute_error: 1.5306 - val_loss: 1.3391 - val_mean_absolute_error: 1.1381\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.8258 - mean_absolute_error: 1.6257 - val_loss: 1.3996 - val_mean_absolute_error: 1.2015\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7426 - mean_absolute_error: 1.5452 - val_loss: 1.3243 - val_mean_absolute_error: 1.1281\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7360 - mean_absolute_error: 1.5403 - val_loss: 1.2619 - val_mean_absolute_error: 1.0679\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7342 - mean_absolute_error: 1.5416 - val_loss: 1.2988 - val_mean_absolute_error: 1.1081\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7884 - mean_absolute_error: 1.5997 - val_loss: 1.4195 - val_mean_absolute_error: 1.2328\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7239 - mean_absolute_error: 1.5380 - val_loss: 1.5130 - val_mean_absolute_error: 1.3284\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7896 - mean_absolute_error: 1.6058 - val_loss: 1.2874 - val_mean_absolute_error: 1.1045\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.7885 - mean_absolute_error: 1.6067 - val_loss: 1.3003 - val_mean_absolute_error: 1.1195\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.6788 - mean_absolute_error: 1.4991 - val_loss: 1.3834 - val_mean_absolute_error: 1.2052\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7376 - mean_absolute_error: 1.5604 - val_loss: 1.2731 - val_mean_absolute_error: 1.0970\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.6786 - mean_absolute_error: 1.5023 - val_loss: 1.3022 - val_mean_absolute_error: 1.1265\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.6966 - mean_absolute_error: 1.5208 - val_loss: 1.2946 - val_mean_absolute_error: 1.1191\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7314 - mean_absolute_error: 1.5568 - val_loss: 1.3954 - val_mean_absolute_error: 1.2223\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7442 - mean_absolute_error: 1.5725 - val_loss: 1.3135 - val_mean_absolute_error: 1.1431\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.6869 - mean_absolute_error: 1.5173 - val_loss: 1.2946 - val_mean_absolute_error: 1.1257\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.7530 - mean_absolute_error: 1.5854 - val_loss: 1.3159 - val_mean_absolute_error: 1.1496\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.7580 - mean_absolute_error: 1.5929 - val_loss: 1.2530 - val_mean_absolute_error: 1.0886\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.6281 - mean_absolute_error: 1.4644 - val_loss: 1.2664 - val_mean_absolute_error: 1.1040\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 1.6524 - mean_absolute_error: 1.4911 - val_loss: 1.3087 - val_mean_absolute_error: 1.1481\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7625 - mean_absolute_error: 1.6024 - val_loss: 1.2828 - val_mean_absolute_error: 1.1236\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.6321 - mean_absolute_error: 1.4736 - val_loss: 1.3200 - val_mean_absolute_error: 1.1621\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.6362 - mean_absolute_error: 1.4792 - val_loss: 1.3800 - val_mean_absolute_error: 1.2239\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 1.7458 - mean_absolute_error: 1.5906 - val_loss: 1.2049 - val_mean_absolute_error: 1.0505\n"
     ]
    }
   ],
   "source": [
    "train = model.fit(\n",
    "    xtrain, \n",
    "    ytrain,\n",
    "    epochs=100, \n",
    "    batch_size=10,\n",
    "    validation_data=(xtest, ytest)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "model.predict(xtest)\n",
    "\n",
    "# accuracy\n",
    "ypred = model.predict(xtest)\n",
    "\n",
    "def accuracy(ytest, ypred):\n",
    "    right = 0\n",
    "    total = 0\n",
    "    for i, x in zip(ytest, ypred):\n",
    "        if(i > x[0]-1 and i < x[0] + 1):\n",
    "            right += 1\n",
    "        total += 1\n",
    "    return right/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6507177033492823"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 11\n",
      "12 11\n",
      "13 12\n",
      "11 12\n",
      "11 8\n",
      "13 13\n",
      "14 13\n",
      "14 12\n",
      "16 16\n",
      "15 15\n",
      "15 15\n",
      "9 9\n",
      "16 15\n",
      "8 8\n",
      "15 16\n",
      "18 17\n",
      "9 9\n",
      "9 9\n",
      "11 10\n",
      "11 12\n",
      "11 12\n",
      "16 16\n",
      "16 16\n",
      "13 14\n",
      "10 12\n",
      "14 15\n",
      "0 10\n",
      "12 12\n",
      "12 11\n",
      "15 15\n",
      "13 13\n",
      "13 11\n",
      "11 11\n",
      "4 5\n",
      "13 12\n",
      "11 11\n",
      "14 15\n",
      "14 13\n",
      "10 10\n",
      "11 11\n",
      "7 7\n",
      "9 9\n",
      "12 11\n",
      "0 4\n",
      "11 11\n",
      "16 16\n",
      "13 13\n",
      "13 11\n",
      "14 13\n",
      "11 12\n",
      "12 11\n",
      "10 11\n",
      "14 11\n",
      "15 13\n",
      "13 13\n",
      "14 14\n",
      "15 16\n",
      "11 11\n",
      "16 17\n",
      "8 6\n",
      "11 11\n",
      "13 14\n",
      "10 11\n",
      "0 9\n",
      "11 10\n",
      "15 15\n",
      "13 12\n",
      "15 15\n",
      "13 12\n",
      "11 12\n",
      "10 11\n",
      "13 13\n",
      "14 15\n",
      "11 11\n",
      "9 9\n",
      "16 17\n",
      "12 11\n",
      "13 12\n",
      "11 12\n",
      "11 10\n",
      "17 16\n",
      "10 10\n",
      "14 14\n",
      "8 10\n",
      "15 13\n",
      "9 8\n",
      "14 12\n",
      "11 11\n",
      "10 8\n",
      "11 12\n",
      "11 12\n",
      "13 15\n",
      "12 12\n",
      "10 10\n",
      "10 8\n",
      "13 13\n",
      "8 7\n",
      "13 14\n",
      "10 9\n",
      "11 10\n",
      "16 16\n",
      "9 9\n",
      "12 12\n",
      "10 9\n",
      "17 16\n",
      "10 10\n",
      "18 17\n",
      "12 12\n",
      "13 13\n",
      "13 13\n",
      "11 11\n",
      "10 12\n",
      "13 13\n",
      "9 11\n",
      "10 12\n",
      "13 13\n",
      "16 16\n",
      "16 15\n",
      "12 13\n",
      "12 12\n",
      "11 9\n",
      "13 13\n",
      "16 17\n",
      "9 8\n",
      "10 10\n",
      "11 11\n",
      "12 13\n",
      "10 11\n",
      "14 14\n",
      "18 17\n",
      "17 17\n",
      "18 16\n",
      "13 13\n",
      "10 10\n",
      "12 13\n",
      "12 10\n",
      "11 11\n",
      "14 13\n",
      "16 16\n",
      "10 11\n",
      "10 11\n",
      "12 11\n",
      "10 10\n",
      "0 6\n",
      "13 12\n",
      "0 9\n",
      "12 10\n",
      "11 10\n",
      "10 9\n",
      "15 14\n",
      "11 9\n",
      "11 10\n",
      "17 17\n",
      "12 12\n",
      "8 5\n",
      "10 11\n",
      "14 14\n",
      "18 17\n",
      "11 11\n",
      "10 10\n",
      "6 8\n",
      "14 14\n",
      "0 7\n",
      "10 10\n",
      "10 9\n",
      "10 12\n",
      "12 10\n",
      "0 -2\n",
      "15 15\n",
      "6 6\n",
      "11 12\n",
      "12 11\n",
      "15 15\n",
      "12 12\n",
      "0 2\n",
      "15 15\n",
      "8 10\n",
      "12 12\n",
      "0 8\n",
      "11 11\n",
      "19 18\n",
      "11 12\n",
      "7 7\n",
      "7 8\n",
      "16 16\n",
      "13 13\n",
      "15 15\n",
      "10 10\n",
      "18 18\n",
      "0 7\n",
      "12 13\n",
      "13 13\n",
      "14 15\n",
      "19 18\n",
      "12 13\n",
      "13 13\n",
      "11 9\n",
      "8 9\n",
      "10 10\n",
      "11 12\n",
      "11 13\n",
      "9 9\n",
      "18 18\n",
      "9 11\n",
      "18 17\n",
      "9 9\n",
      "13 12\n",
      "7 6\n",
      "12 12\n"
     ]
    }
   ],
   "source": [
    "for i, x in zip(ytest, ypred):\n",
    "    print(i, round(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 11.295912\n",
      "12 10.878537\n",
      "13 12.413212\n",
      "11 12.461786\n",
      "11 7.777874\n",
      "13 12.615528\n",
      "14 13.127453\n",
      "14 12.003542\n",
      "16 15.9162\n",
      "15 15.285161\n",
      "15 14.913845\n",
      "9 9.0946045\n",
      "16 15.124886\n",
      "8 7.8360744\n",
      "15 15.790635\n",
      "18 16.56025\n",
      "9 8.631863\n",
      "9 9.370045\n",
      "11 10.1228485\n",
      "11 12.285107\n",
      "11 11.574861\n",
      "16 15.906021\n",
      "16 16.135456\n",
      "13 14.038907\n",
      "10 11.763596\n",
      "14 14.696121\n",
      "0 9.731533\n",
      "12 12.048187\n",
      "12 11.335095\n",
      "15 14.613846\n",
      "13 13.030525\n",
      "13 11.490156\n",
      "11 10.759514\n",
      "4 5.2035704\n",
      "13 11.841465\n",
      "11 10.896719\n",
      "14 14.75732\n",
      "14 12.636949\n",
      "10 10.010921\n",
      "11 10.994352\n",
      "7 7.3425303\n",
      "9 8.588796\n",
      "12 11.340759\n",
      "0 3.9149523\n",
      "11 10.890598\n",
      "16 15.84423\n",
      "13 12.59436\n",
      "13 11.11836\n",
      "14 13.309439\n",
      "11 11.82359\n",
      "12 11.022249\n",
      "10 10.673183\n",
      "14 11.293715\n",
      "15 12.952293\n",
      "13 12.923233\n",
      "14 14.327515\n",
      "15 15.937141\n",
      "11 10.815065\n",
      "16 16.59523\n",
      "8 6.4414735\n",
      "11 10.626641\n",
      "13 13.569984\n",
      "10 10.500208\n",
      "0 9.46936\n",
      "11 10.284539\n",
      "15 15.415836\n",
      "13 11.840929\n",
      "15 14.912266\n",
      "13 12.395558\n",
      "11 11.983296\n",
      "10 11.302204\n",
      "13 12.684507\n",
      "14 14.5359745\n",
      "11 10.975203\n",
      "9 8.606224\n",
      "16 16.564255\n",
      "12 10.7860155\n",
      "13 11.998035\n",
      "11 11.768808\n",
      "11 10.397573\n",
      "17 15.679603\n",
      "10 9.670267\n",
      "14 14.229794\n",
      "8 9.508734\n",
      "15 13.47968\n",
      "9 8.295411\n",
      "14 12.335316\n",
      "11 11.218624\n",
      "10 7.8198295\n",
      "11 11.559912\n",
      "11 11.505028\n",
      "13 14.613137\n",
      "12 11.922825\n",
      "10 10.121099\n",
      "10 8.423474\n",
      "13 13.479875\n",
      "8 6.897195\n",
      "13 13.646187\n",
      "10 9.107527\n",
      "11 9.587071\n",
      "16 15.537067\n",
      "9 9.488076\n",
      "12 11.603996\n",
      "10 9.37329\n",
      "17 16.1484\n",
      "10 9.525019\n",
      "18 17.24572\n",
      "12 11.536496\n",
      "13 13.350033\n",
      "13 12.936529\n",
      "11 11.003727\n",
      "10 12.088099\n",
      "13 12.620442\n",
      "9 11.38679\n",
      "10 11.9347515\n",
      "13 12.915777\n",
      "16 16.033606\n",
      "16 14.709418\n",
      "12 12.9608965\n",
      "12 12.329852\n",
      "11 8.706187\n",
      "13 12.726625\n",
      "16 16.95768\n",
      "9 8.1833515\n",
      "10 9.837773\n",
      "11 10.996847\n",
      "12 13.114719\n",
      "10 11.197828\n",
      "14 14.130648\n",
      "18 16.955774\n",
      "17 17.18195\n",
      "18 15.957739\n",
      "13 13.371523\n",
      "10 10.107578\n",
      "12 12.506338\n",
      "12 10.406229\n",
      "11 11.299894\n",
      "14 12.549841\n",
      "16 16.113352\n",
      "10 11.345385\n",
      "10 11.478453\n",
      "12 10.827077\n",
      "10 10.127297\n",
      "0 6.199367\n",
      "13 11.91729\n",
      "0 8.591998\n",
      "12 9.615486\n",
      "11 10.143425\n",
      "10 9.354319\n",
      "15 14.012891\n",
      "11 9.170015\n",
      "11 9.79303\n",
      "17 16.791807\n",
      "12 12.01411\n",
      "8 5.486746\n",
      "10 10.7756\n",
      "14 14.098017\n",
      "18 17.263245\n",
      "11 11.008255\n",
      "10 9.856968\n",
      "6 7.542612\n",
      "14 14.198687\n",
      "0 6.6620455\n",
      "10 9.651537\n",
      "10 8.541839\n",
      "10 11.865134\n",
      "12 10.462461\n",
      "0 -1.5473988\n",
      "15 15.158789\n",
      "6 6.2990837\n",
      "11 11.930397\n",
      "12 11.444876\n",
      "15 14.682432\n",
      "12 11.65934\n",
      "0 1.83812\n",
      "15 14.723882\n",
      "8 9.919939\n",
      "12 12.199701\n",
      "0 8.334222\n",
      "11 11.336018\n",
      "19 18.122433\n",
      "11 11.884497\n",
      "7 7.013063\n",
      "7 8.209747\n",
      "16 16.418066\n",
      "13 12.8846\n",
      "15 15.229256\n",
      "10 10.317009\n",
      "18 18.40146\n",
      "0 6.9770527\n",
      "12 12.79771\n",
      "13 12.69354\n",
      "14 14.502827\n",
      "19 17.838757\n",
      "12 12.724043\n",
      "13 13.303648\n",
      "11 8.963385\n",
      "8 9.019597\n",
      "10 10.2509\n",
      "11 12.498964\n",
      "11 12.901041\n",
      "9 8.843882\n",
      "18 18.484962\n",
      "9 10.584616\n",
      "18 17.110258\n",
      "9 9.392332\n",
      "13 11.5251875\n",
      "7 5.920983\n",
      "12 12.050512\n"
     ]
    }
   ],
   "source": [
    "for i, x in zip(ytest, ypred):\n",
    "    print(i, x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c815d5522d730692d6fc1bd8cf8011d5716afbe816a33b6bfcadd78ff6b33837"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
